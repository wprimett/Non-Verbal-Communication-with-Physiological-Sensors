\section{Preliminary Actions I}
\label{cha:Preliminary_Actions_sens_act}

The following section will report on the outcomes contained in the publication that was delivered during the doctorate programme, including personal contributions as respective co-authors: 

\printpublication{alfaras_biosensing_2020}

% Alfaras, M.; Primett, W.; Umair, M.; Windlin, C.; Karpashevich, P.; Chalabianloo, N.; Bowie, D.; Sas, C.; Sanches, P.; Höök, K.; Ersoy, C.; Gamboa, H. Biosensing and Actuation—Platforms Coupling Body Input-Output Modalities for Affective Technologies. Sensors 2020, 20, 5968. https://doi.org/10.3390/s20215968

\section{Biosensing and Actuation Couplings: An Overview of Concepts and Technical Resources}

\subsubsection{Introduction}

In this Section, we present an overview of biosignals that have become standard in low-cost physiological monitoring and show how these can be matched with interaction design concepts for facilitating \textit{bodily engagement} and \textit{aesthetic experiences}. First-person soma design lets researchers look afresh at biosignals that, when experienced through the body, are called to reshape affective technologies with novel ways to interpret biodata, feel it, understand it and reflect upon our bodies. Taking both strands of work together offers unprecedented design opportunities that inspire further research. Through first-person design, an approach that draws upon the designer’s felt experience and puts the sentient body at the forefront, we outline a comprehensive work for the creation of novel interactions in the form of couplings that combine biosensing and body feedback modalities of relevance to affective health. These couplings lie within the creation of design toolkits that have the potential to render rich embodied interactions to the designer and user alike. As a result we introduce the concept of “orchestration”. By orchestration, we refer to the design of the overall interaction: coupling sensors to actuation of relevance to the affective experience; initiating and closing the interaction; habituating; helping improve on the users’ body awareness and engagement with emotional experiences; soothing, calming, or energising, depending on the emotional and social context and the intentions of the designer. The findings of these explorations are moralised as a set of experimental sensor-actuation couplings to gain a broader perspective on designing embodied systems.

\subsubsection{Somaesthetic Design in Practice}

We take somaesthetic design—a design stance that draws upon the felt body and takes inspiration from experiencing it—and then combine it with the innovative integration of biosensors and actuators. The disruptive somaesthetic view, moves away from the idea of monitoring the body for the sake of bad habit reduction in pursuit of a healthy and long life \cite{hook_somaesthetic_2015}. Soma design, rather, lets us get attuned to our bodies and use sensations as a valuable resource instead of something to be improved to meet performance standards. In this context, we present novel research on embodied interaction design couplings, that is, sensing-actuation combinations of aesthetically evocative body input-output modalities that render biodata shareable, body-centered, highly tangible or even able to be experienced collectively.

With a focus on the body, design research is used as a way to enter introspectively to emotional self-reflection and potentially disrupt the way we relate to our mental well-being with technology-mediated interactions. Technology-mediated interactions, drawing upon ubiquitous computing capabilities could add novelty and be taken further to psychotherapy contexts. Using technology for sensing and actuating upon our body, we can get access to bodily states from our physiology to then act in such a way that we help to alter or reassess our psychophysiological states. This construction process may be developed to extend our knowledge and expectations regarding the internal mechanics of our own body and serves as a bridge to design better informed affective health technologies. This comprises the first-person design stance, the somaesthetic design (“design with the body”) approach \cite{hook_designing_2018}, and the path to orchestration.

\subsection{Respiratory Sensing}

Respiration sensors monitor the inhalation-exhalation cycles of breathing, i.e. the process to facilitate the gas exchange that takes place in the lungs. In every breathing cycle, the air is moved into and out of the lungs. A breathing sensor uses either piezoelectric effects on bendable wearable bands or accessories (one of the most predominantly used technologies), respiratory inductance plethysmography on wired respiration bands around the thorax, microphones on the nose/mouth airflow, plethysmographs (measuring air inflow) or radiofrequency, image and ultrasonic approaches. A review on breathing monitoring mechanisms is found at \cite{massaroni_contact-based_2019}. For piezoelectric breathing sensors, thoracic or abdominal displacements produced in breathing cycles bend a contact surface that converts ressistive changes to continuous electrical signals.

A piezoelectric breathing sensor is usually located on the thoracic cavity or the belly, using a wearable elastic band. With adjustable strap and fastening mechanisms, the sensor can be placed slightly on one side where bending is most relevant, optimizing the use of the sensor range. These kinds of sensors, allow both the study of thoracic and abdominal breathing. With the development of conductive fabric, breathing sensors are making its way into the smart garment market in the form of T-shirts and underwear bands. Breathing is a relatively slow biosignal, with breathing rates often below 20 inhale/exhales per minute. A sampling rate frequency as low as 50Hz is sufficient to capture the dynamics of respiration.

A breathing signal informs the respiration dynamics of the subject, i.e. the dynamics of the process mediating gas exchange in the lungs. The monitoring of an innate breathing action brings in the assessment of breathing cycles and rates which can be used to handle problems involving breathing interruptions, oxygen intake, metabolism of physical activity, as well as emotional stressors. In terms of analysis, breathing cycles are studied using breathing rates, the maximum relative amplitude of the cycle, inhale-exhale volume estimation, inhale-exhale duration, and inspiration depth, that allow the characterisation of several breathing patterns. From an incoming data stream, it's possible to discern the Respiration rate, Inspiration duration expiration duration, Inspiration-expiration ratio, and Inspiration depth.

While piezoelectric breathing sensors are prominent given the low cost and form factor advantages of wearable sensor platforms, deviations in placement have an effect in the relative range of the response signal. Movement artefacts, most relevant when physical activity is present, are a common source of problems. Respiration sensing techniques like the respiratory inductance plethysmography, compensate the highly localised piezoelectric approach with a sensor capturing the general displacement of the whole thoracic cavity, yielding a signal less prone to movement artefacts. The monitoring of breathing cycles is usually accurate, although the exploration of effects to be used as voluntary inputs in interactions, such as holding the breath, are not easily captured. 

\subsection{Surface electromyography (sEMG)}
% \noindent \textbf{How it works:}
The recording of the electrical activity produced by skeletal muscles receives the name of electromyography (EMG). Human muscles are made up of groups of muscle units that, when stimulated electrically by a neural signal, produce a contraction. The recording of the electrical activity of the muscles (voltage along time), traditionally relying on intrusive needle electrodes (intramuscular), is easily accessible nowadays by means of surface electrodes that capture the potentials of the fibers they lay upon. The result of this measurement is a complex surface electromyography signal (sEMG) that reveals data about movement and biomechanics of the contracted muscles (see figure \ref{fig:biosignals}a). 

% \noindent \textbf{What:}
Electromyography signals inform about the contraction of specific muscles and parts of the body. The EMG signal consists in the time representation of rapid voltage oscillations. Its amplitude range is approximately 5mV. The EMG signal grants an assessment of several aspects of a physical activitys such as muscle contraction duration, the specific timing at which movements or contractions are activated, the presence of muscular tension or fatigue, and the extent to which different fibers (area) are contracted. The analysis is conducted through noise filtering, together with feature extraction that yields contraction onset detection, the estimation of signal envelopes, and the computation of average frequencies.This lets subjects deepen their understanding of movement strategies, very relevant for embodied art and sports performance, improve muscle coordination, or even reveal existing movement patterns that they are unaware of.

% \noindent \textbf{Features:} Onset instants; Max amplitude; Instant of maximum amplitude; Activation energy; Envelope;

% \noindent \textbf{Where:}
Having become the standard in EMG monitoring, bipolar surface electrodes consist of three electrodes. Two of them (+/-) must be placed close to each other, on the skin that lies on top of the muscle under study, along the fibers' direction, while the third one is placed in a bony area where no muscular activity is present. This allows the measurement of electrical potential differences with respect to a common reference, yielding a unique signal that represents the muscular activity of the area. Given the fast muscle-neural activation nature of EMG signals and the presence of different active muscles contributing to the same signal, muscle activity must be acquired at sampling rates no lower than 200Hz frequencies. Working at 500Hz is desirable, while a sampling rate of 1000Hz guarantees the tracking of all the relevant events at a muscular level.

Surface EMGs are intrinsically limited to the access to superficial muscles. This is compromised by the depth of the subcutaneous tissue at the site of the recording which depends on the weight of the subject, and cannot unequivocally discriminate between the discharges of adjacent muscles.
Proper grounding (reference electrode attached to a bony inactive muscular region) is paramount to obtain reliable measurements. Motion artifacts and muscular crosstalk compromise the assessment of the muscle activity under study. In this context, interference from cardiovascular activity is not uncommon, particularly in areas such as chest and abdomen. sThe presence of power supplies and mains (powerline) in the vicinity poses the risk of 50Hz-60Hz interference.

\subsection{Actuation Mechanisms} 

The mechanisms to provide actuation in a form of feedback to the human take an important role in creating a complete interaction from sensing body properties to making the subject aware of them. Our research aims at linking biosensing to body actuation. Actuation is generally provided by mechanical elements that move and respond to input signals in order to either control or inform about a system. We stretch this definition to include feedback mechanisms such as screen-based visuals, although no mobile mechanical element is necessarily implied. In this section, we focus on actuation mechanisms that can be easily controlled and coupled to our body. We take a similar approach to the structure used to describe the biosignals, on explaining: how, what, where, when, and the actuator limitations and usage precautions for a selected list of actuators. The range of actuation mechanisms presented draws upon our research on affective technologies and interaction design, as well as inspirational works present interaction design research, but it should be seen as a non-exhaustive list of possibilities.

\subsubsection{Visual Feedback}

% \noindent \textbf{How it works:}
Visual biofeedback is the representation of bodily signals over time, capable of informing physiological changes, initiated from within the body. Examples of this could be ECG feedback, respiration feedback, or movement tracking, usually employed in health metrics or sports performance research. Screen-based systems for biosensor feedback are standard practice in clinical settings and hospitals, purposed as a means to assess the dynamics of the aforementioned changes, helping to gain understanding and tracking the inner state of a given subject. Biofeedback use has for instance been adopted in psychotherapy, as research suggests that the technique provides a mechanism to self-regulate the emotions. 

Screen-based visual biofeedback can be projected from a 2D graphical a display, either a computer screen or a designated sensing platform display. This modality benefits from direct control over light, colours, strokes, and visual styles to represent a changing signal that evolves with time. Signal peaks and troughs appear in an axis showing the measurement magnitude in a given range, so that rapid and slow dynamics can be seen as the representation moves along the time axis when updated. 

To facilitate a live convergence between the input and feedback, it is important that the represented signals are updated in real-time. Doing otherwise, although possible using delays or technology limitations, would compromise the ability of the actuation to convey the tracking meaning attributed to the practice of biofeedback. When sensing requirements pose concerns on the technical ability to render a smooth representation through time, approaches such as averaging or undersampled representations are used. 

Screen-based visual biofeedback connects easily with the mathematical properties that underlie the signals under study. However, signal processing procedures such as filtering, scaling or normalisation are crucial in achieving a smooth and flowing representation. These come, of course, tightly dependent on the available resources in computational power.
There are situations in which feedback users report finding difficulties or experiencing anxiety when engaging in the assessment of body rhythms. Moreover, visual information tends to remarkably capture the attention of the user, thus needing special care when used as an element of broader interaction (movement, performance, exercise) that could render a poorer experience quality or present a deviation from the aimed activity.

\subsubsection{Sound Feedback}

Sound feedback, when applied to biosignals, is the audio representation of body signals that uses sound properties to inform about body changes happening along time. Its goal is to exploit our sophisticated trained sense of hearing to convey meanings linked to body signal features, leading to the understanding and tracking of a given subject’s biosignal dynamics.

Sound feedback uses the properties of sound, i.e. volume, pitch or frequency (note), rhythm, harmony, timbre, and transients (attack, sustain, etc.) among others, to represent a signal (or its features) that changes over time. Its generation, often using speakers or headphones, is linked to properties of the signal. Alternative approaches draw upon several transducing paradigms, i.e. different ways to convert electrical signals into sound (electromechanical as in the case of speakers, piezoelectric or others), often more limited such as buzzers or beepers made of basic vibrating elements that produce sound. 

Sound can be produced by speakers when amplifying electrical pulses to audible vibrations, allowing users to listen to the feedback without the need for additional equipment. Headphones, working by the same principle, can be used for the same purpose but only providing feedback to the person wearing them. The human hearing range typically comprises frequencies between 20Hz and 20000Hz. The oscillating frequency of the sound wave that is created is what gives it a particular tone (what we call a note). The different times at which sound waves are generated is what creates the meaning of rhythm and articulation.

Audio generation and processing techniques are complex. Whilst high-level hardware and software tools can be exploited to make a complete system more accessible, there certainly remains a relevant learning-curve. The scenario in which audio feedback is deployed conditions a lot the effect achieved, given the fact that materials surrounding the sound generating system at use impose effects like reverberation, echoes, or absorption. Exposure to sound feedback for a prolonged period of time has some drawbacks. Excessive volumes can be harmful one´s auditory system, to the point of irreversible hearing impairment. Sound elements that lack textural richness (e.g. a pure sine-wave) may limit long-term engagement from users or potentially cause irritation. 

\subsubsection{Vibrotactile Actuation}

Vibrotactile actuation uses motors to stimulate communication utilizing touch sensitivity, and more precisely tactile vibrations. When linked to physiological sensory, the vibration feedback can be used used to convey features of the biosignal being tracked. This mechanism is built upon motors, which can mostly be categorised under two types: Eccentric rotating mass vibration motor (ERM), and Linear resonant actuator (LRA). These actuators usually enclosed within a small capsule with simple positive and negative ($+/-$) terminals to be driven. The typical power requirements for these components normally range between 1 Volts and 5 Volts. 

With weights below 1g, the small form factor of these actuators makes them suitable for body explorations, often relying on patches, elastic bands, or holders. Typical uses include also vibrotactile-equipped wristbands or smartwatches. Besides the traditional game/remote controllers including vibrotactile feedback and actuating on the hands, the currently ubiquitous role of mobile phones has spread the use of vibration feedback and patterns for notification, alarms and other communication examples anywhere a phone can be placed or hold. Small vibrotactile motors feature fast startup and breaking times and can actuate taking rotations up to 11000 revolutions per minute (RPM), in the case of ERMs, and oscillations of the order of few hundreds of Hertz. 

Vibration comes often with undesired noises or sounds. While this is mitigated by rubber-made absorbing structures often integrated in the motors, use cases need to consider this aspect. While vibrotactile actuation offers the opportunity to explore a particular type of haptic feedback, the use of small motors limits the generated effects, in terms of amplitude, duration, and intensity perceived. To create vibration sequences, several motors are needed, provided integration software and hardware development efforts are carried out. The actuators often require extra drivers to widen the operating regime possibilities while maintaining electrical safety standards. As generally advised in the case of feedback modalities applied to the body, haptic feedback actuation has to go hand in hand with user experience studies, since prolonged exposure and certain placements can lead to discomfort

\subsubsection{Temperature Actuation}

\subsubsection{Shape-Changing Actuation}



\subsection{Orchestration} 

By orchestration, we refer to the design of the overall interaction: coupling sensors to actuation of relevance to the affective experience; initiating and closing the interaction; habituating; helping improve on the users' body awareness and engagement with emotional experiences; soothing, calming, or energising, depending on the affective health condition and the intentions of the designer. 

Orchestration mechanisms try to achieving the overarching goal of linking body measurements to meaningful representation through actuation that addresses various modalities that act on our bodily senses, be it visual, hearing, touch or otherwise. To achieve this facilitation it is crucial to be able to combine and nicely coordinate the relationship between input (sensors) and output (actuators). In this scope, the term orchestration defines the process of: 
\begin{itemize}
    \item Creating couplings, that is, combining biosensors and actuators in place 
    Coordinating the technology-mediated body interactions 
    \item Working on the sequence in which different modalities are addressed via the sensors and actuators. Deciding which one goes first 
    \item Understanding the design possibilities, acknowledging affordances, limitations and roles of the involved technologies that involved in these interactions 
    \item Consolidate the process of introducing a new user to the experience, defining a balance between explicit interaction or open exploration
    \item Retrieving meaning information from the input data, potentially laying out machine learning, feature extraction, smart event recognition, or signal processing tools that can be applied to render the interactions more intuitive
\end{itemize}

\subsection{Breathing in Synchrony: From Physiological Synchrony to Audio Feedback}

\subsubsection{Synchrony as a Basis for Collective Engagement}

Synchrony in neural and physiological activity has been hypothesised to be a mechanism for empathy, mutual understanding, and trust \cite{wallot_beyond_2016}. It can describe a rhythmic coordination of movements and biological behaviour to an external signal \cite{koole_synchrony_2016}. Synchrony can be present in different relational contexts, such as therapist to patient \cite{koole_synchrony_2016}, couples \cite{karvonen_sympathetic_2016}, close friends \cite{miles_birds_2011}, as well as mother and child \cite{butler_emotional_2013}. Synchrony have certain positive effects on individual’s well-being. For example, leading people to synchronous movements facilitate cooperative behaviour \cite{kirschner_joint_2010}, potentially linked to increases in self-esteem, compassion and rapport \cite{lumsden_sync_2014, fujiwara_rhythmic_2020}, moving in synchrony also leads participants to greater perceptual sensitivity to movements \cite{zamm_endogenous_2016}.

\subsubsection{The Role of Respiration in Emotional Regulation}

While searching for practices beneficial for affective well-being, breathing exercises were also considered. Recent research indicates that many of the detrimental effects of negative emotional states and sympathetic dominance of the autonomous nervous system can by counteracted by different forms of meditation, relaxation, and breathing techniques. In fact, meditation and breathing techniques can reduce stress, anxiety, depression, and other negative emotional states [14,15,24,68].

\subsubsection{Sensory Input and Processing Strategies}

This coupling example, drawing upon the psychology concept of therapeutic alliance \cite{koole_synchrony_2016}, takes respiration data from two users in the same physical space, where two BITalino devices stream data wirelessly to a host computer. In this example, two users participate in a timed breathing exercise together whilst their individual respiratory patterns are being measured with piezoelectric (PZT) bands placed around the diaphragm (see Figure X.X). The data is aggregated on the host computer, executing a script that measures the collective breathing activity. From here, we apply shared biofeedback in the form of sound to stimulate synchrony awareness and physiological dialogue between users over time.

The exploration followed a stage of preliminary research on physiological synchrony features drawn from published research drawing upon statistical measurements, potentially generalise to signals other than breathing \cite{zamm_endogenous_2016}. We implemented the computation of linear regression coefficients, cosine similarity and correlations between filtered signal and derivatives. The process for mapping the user’s activity audio output can be split into two main components. First, the sensory data is transmitted to a data processing server, which is used to perform statistical analysis on the incoming signals, calculating a “magnitude of synchrony” using the features listed above. After a fifteen second warm-up period, the system accumulates a sufficient amount of data to determine mutual behaviour, and the resulting values are encoded into Open Sound Control (OSC) messages that are continuously streamed to a local address, enabling the designer to map the data to appropriate parameters for sound feedback. With this generic protocol in place, we aim to embrace modularity, and advocate for the experimentation of sonic associations. In our tests, we used Cecilia’s [ref] built-in granular synthesis engine; this manipulates the playback of a pre-recorded soundscape divided into independent samples of 10 to 50 milliseconds \cite{roads_introduction_1988}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Chapters/Figures/Feature_Extraction_Sync_MAX.jpg}
	\caption{Feature Extraction GUI for Synchrony Metrics}
	\label{fig:Concept_Venn}
\end{figure}

\subsection{Inflatable Shapes, Early Progressions}

The first iteration of inflatable shapes had difficulties with fast deflation. This was partly solved by adding another air pump, devoted specifically for exhausting air. Experimenting with the addition of separators inside an inflatable shape or splitting it into multiple inflatable sections may further improve exhaust performance. Another limitation is unavoidable minor air leakage, which will happen due to imperfections in manufacturing the actuator (sealing inflatable shape, valve timing).

\subsection{Machine Learning and Physiological Sense-Making}

A relevant part of the research presented here has focused on the extension of biosignal feature extraction, processing, and real-time analysis (e.g. to potentially enrich real-time feedback possibilities within the lab and in more ecological settings).  An novel aspect in this research, is the possibility to address machine learning interactively. This research receives the name of Interactive Machine Learning. The interest in working toward the deployment and refinement of real-time machine learning processing of information is stressed throughout the thesis. However, the interactive aspect is of relevance, not only when exploring preprogrammed effects in multi-user experiences, but also when deliberately working to alter effects and come up with novel biodata-actuation mappings. This echoes early research achievements such as Wekinator [Fiebrink, 2011] and puts forth recent progress such as that exhibited by the Teachable Ma- chine [Carney et al., 2020]. Interactive machine learning [Fails and Olsen, 2003], originally conceptualised as a way to correct machine learning classifications on the fly has evolved significantly. By means of tools like Wekinator or Teachable Machine-like paradigms, it is possible to define actions, classes of images, movements or gestures and link them to a classification output effect. 

\subsection{Adjustment and Tweakability for User Empowerment}
% User Empowerment though customisation 
We describe how embodied sensor technologies react differently in accordance with the unique biological characteristics of the body. Similarly, the perceived impact of a given actuation mechanism—as those described in Section 4—largely depends on the sensitivity to a given stimulus, as well as the natural bodily variations between different users. With this considered, we recognise the necessity to attune the system’s parameters in order to produce mappings that facilitate meaningful interactions that are not overly obtrusive. While auto-calibration mechanisms have been implemented in the previous examples, which typically define and minimum/maximum parameter ranges, we foresee an extended benefit in adopting Interactive Machine Learning (IML) \cite{fails_interactive_2003,amershi_power_2014} frameworks as means to foster perspectives respecting body pluralism. Furthermore, we set out explore the use of Interactive Machine Learning to develop novel coupling relationships that go beyond linear mappings, as well as intuitive mappings between multimodal inputs and multi-dimensional outputs. In our sound-based examples, visual programming environments heavily assisted the orchestration process. In both cases, the systems enabled users to visualise a continuous stream of mappable data in real-time, clearly exposing any unexpected behaviour that may occur (for example, with the displacement of sensor electrodes). The node-based functionality of the frameworks allowed for a coherent representation of the dataflow and signal processing steps in order of execution, less abstract compared to a code-based script. During the process of developing the system, a user interface is generated in parallel on-the-fly as each node presents a GUI element that grants the designer access to parameters such as scaling and smoothing coefficients. In Section X.X, a basic interface allowed users to test and compare a set of algorithms for sound mappings. This workflow can be beneficial for rapid experimentation with a variety of parameters and signal processing techniques that influence the interactive experience. It also presents a convenient solution for fine-tuning a complete system according to the user’s experience.

\section{Latency Assessments when Using Multiple Acquisition Sources}

\subsection{Motivation and Objective}

Technological advancements in the field of sensory devices have allowed the collection of various bodily signals such as Acceleration (ACC), Electrocardiogram (ECG), Electromyogram (EMG), and Electrodermal Activity (EDA) using discrete wearable devices. These data streams can be used to validate semantic emotional descriptors based on valence and arousal measurements, linked to the user’s involuntary reactions transmitted by the Autonomic Nervous System (ANS). Where researchers may need to acquire data from many sources at once, we evaluate the use of the BITalino R-IoT, a low-cost sensory device designed for monitoring physiological activity in real-time.

A crucial feature we want researchers to be able to exploit is the ability to acquire and process data from multiple devices simultaneously. This can be configured to monitor the activity of many users or even to place sensors on additional body parts (to track movement from different limbs, for example). To benchmark the hardware capabilities of the device in experimental environments, we carry out tests to evaluate performance, outlining the absolute and relative latency in different conditions. These tests consider the effect of including additional devices to the network and differences in wireless range of data transmission.

\subsection{Preliminary Results}

In our tests, we calculate the time taken for the host computer to receive a response to a stimulus which changes in the analog input on the R-IoT device,which continuously sends data back to the host computer over a shared Wi-Fi network using the Open Sound Control (OSC) protocol. The stimulus signal is distributed to multiple devices which are included in the network to see the effect this has on latency. In addition, the test the impact of increasing the wireless communication distance from 1 metre to 5 metres. The test is run for a duration of several minutes to assess how the latency deviates over time.
Results: As a base-latency, using one device at a placed a metre away from the wireless receiver, we calculated a mean latency of 8.9ms. This increased to 10.7ms as the distance increased to 5 metres. When we included 4 devices, we saw a 1.1ms average increase in latency at 1 metre and 1.3ms at 5 metres. The average difference in latency among the devices was calculated as 3.2ms and 5.9ms respectively. The latency measurements were deemed stable over the time period of the test with no observable trend. A complete analysis has been published on the BITalino R-IoT documentation page.

We found a slight increase in latency when using more devices on one network and as we increased the distance. Whilst this should be taken account for data synchronisation tasks, we would consider the BITalino R-IoT to be a suitable device for studies collecting physiological data from multiple sources. Our tests also support the use of the OSC protocol for this purpose. 